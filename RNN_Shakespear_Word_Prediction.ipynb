{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T01:29:39.706597600Z",
     "start_time": "2025-09-27T01:29:35.963461300Z"
    }
   },
   "id": "ea9884d6bf230982"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T01:29:39.722263500Z",
     "start_time": "2025-09-27T01:29:39.722263500Z"
    }
   },
   "id": "45f797b548c22d6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "print(text[:500])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.722263500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize by whitespace (simple word-level tokenizer)\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "\n",
    "stoi = {w: i for i, w in enumerate(vocab)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def encode(seq): return [stoi[w] for w in seq.split() if w in stoi]\n",
    "def decode(idxs): return \" \".join([itos[i] for i in idxs])\n",
    "\n",
    "data = torch.tensor([stoi[w] for w in words], dtype=torch.long)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Encoded sample:\", data[:20])\n",
    "print(\"Decoded back:\", decode(data[:20].tolist()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.738334100Z"
    }
   },
   "id": "8afd5edb28093f17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "split1 = int(0.1 * len(data))   \n",
    "split2 = int(0.9 * len(data)) \n",
    "test_data = data[:split1]\n",
    "train_data = data[split1:split2]\n",
    "val_data  = data[split2:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-27T01:29:39.864048Z",
     "start_time": "2025-09-27T01:29:39.754011400Z"
    }
   },
   "id": "9d57e5bd6fab716a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        logits = self.fc(out)\n",
    "        return logits, h\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.754011400Z"
    }
   },
   "id": "79356911c0e19349"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, batch_size, device):\n",
    "    ix = torch.randint(0, len(data) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.769618900Z"
    }
   },
   "id": "a26e39fa060e8d0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(model, data, seq_len, batch_size, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        steps = len(data) // (seq_len * batch_size)\n",
    "        for i in range(steps):\n",
    "            start = i * seq_len\n",
    "            x = data[start:start+seq_len*batch_size].view(batch_size, seq_len).to(device)\n",
    "            y = data[start+1:start+1+seq_len*batch_size].view(batch_size, seq_len).to(device)\n",
    "\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            total_tokens += y.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return avg_loss, perplexity.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.785303Z"
    }
   },
   "id": "b464c38493fb9188"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "seq_len = 10\n",
    "num_epochs = 5  \n",
    "lr = 0.0005\n",
    "batch_size = 64\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "model = WordRNN(vocab_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for step in range(steps_per_epoch):\n",
    "        x, y = get_batch(train_data, seq_len, batch_size, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / steps_per_epoch\n",
    "    train_ppl = torch.exp(torch.tensor(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_ppl = evaluate(model, val_data, seq_len, batch_size, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | \"\n",
    "          f\"Train Loss {avg_train_loss:.4f} | Train PPL {train_ppl:.2f} | \"\n",
    "          f\"Val Loss {val_loss:.4f} | Val PPL {val_ppl:.2f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_word_rnn.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience counter = {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model before final test\n",
    "model.load_state_dict(torch.load(\"best_word_rnn.pt\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.785303Z"
    }
   },
   "id": "9c6d3a7c86635cd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_ppl = evaluate(model, test_data, seq_len, batch_size, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Perplexity: {test_ppl:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.800916200Z"
    }
   },
   "id": "15abc2392d9df101"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate(model, start=\"ROMEO\", length=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[start]]], device = device)\n",
    "    h = None\n",
    "    out = [start]\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits, h = model(idx, h)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx = torch.multinomial(probs, num_samples=1).to(device)\n",
    "        word = itos[idx.item()]\n",
    "        out.append(word)\n",
    "    return \" \".join(out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.800916200Z"
    }
   },
   "id": "4c6c2f56fd1c1b5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(generate(model, start=\"ROMEO:\", length=20, temperature=0.8))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-27T01:29:39.800916200Z"
    }
   },
   "id": "565f1086fb1f2ef1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment on Validation Results\n",
    "\n",
    "We observe that while **training loss and perplexity decrease rapidly**, the **validation loss increases sharply** across epochs.  \n",
    "This indicates that the model is **overfitting** to the training set and failing to generalize.  \n",
    "\n",
    "The main reason is that our dataset is **too small for word-level modeling**:\n",
    "- The Shakespeare corpus contains **tens of thousands of unique words** (large vocabulary).\n",
    "- Many of these words appear only a handful of times.  \n",
    "- The model can memorize frequent word sequences in the training split, but it has **no statistical basis** for predicting rare or unseen words in the validation split.  \n",
    "- Cross-entropy punishes these wrong but confident predictions very harshly, causing validation perplexity to explode.\n",
    "\n",
    "In other words:  \n",
    "The bad validation results are not a bug in the training loop, but a **consequence of data scarcity relative to vocabulary size**.  \n",
    "\n",
    "This is why **character-level modeling** (small vocab, dense repetition) or **subword tokenization** (BPE/WordPiece) is generally used on such small corpora.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c544795f3c8dbb1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "22e2735eebc2461d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
